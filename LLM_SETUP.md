# LLM Kurulum Rehberi

Bu sistem, AI destekli aÃ§Ä±klamalar iÃ§in Ã§eÅŸitli LLM saÄŸlayÄ±cÄ±larÄ±nÄ± destekler.

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

### SeÃ§enek 1: OpenAI (Ã–nerilen - En Ä°yi SonuÃ§lar)

1. **API Key AlÄ±n**
   - https://platform.openai.com/api-keys adresinden API key oluÅŸturun

2. **Paketi YÃ¼kleyin**
   ```bash
   pip install openai
   ```

3. **Ortam DeÄŸiÅŸkenlerini AyarlayÄ±n**
   ```bash
   # Windows
   set OPENAI_API_KEY=your-api-key-here
   set LLM_PROVIDER=openai

   # Linux/Mac
   export OPENAI_API_KEY=your-api-key-here
   export LLM_PROVIDER=openai
   ```

4. **VarsayÄ±lan Model**: `gpt-4o-mini` (maliyet-etkin)
   - FarklÄ± bir model kullanmak iÃ§in `backend/llm_client.py` dosyasÄ±nÄ± dÃ¼zenleyin

### SeÃ§enek 2: Anthropic Claude

1. **API Key AlÄ±n**
   - https://console.anthropic.com/ adresinden API key oluÅŸturun

2. **Paketi YÃ¼kleyin**
   ```bash
   pip install anthropic
   ```

3. **Ortam DeÄŸiÅŸkenlerini AyarlayÄ±n**
   ```bash
   export ANTHROPIC_API_KEY=your-api-key-here
   export LLM_PROVIDER=anthropic
   ```

### SeÃ§enek 3: Ollama (Yerel, Ãœcretsiz) - **Ã–NERÄ°LEN**

**Otomatik Kurulum (Kolay):**

Windows:
```bash
setup_ollama.bat
```

Linux/Mac:
```bash
chmod +x setup_ollama.sh
./setup_ollama.sh
```

**Manuel Kurulum:**

1. **Ollama'yÄ± YÃ¼kleyin**
   - https://ollama.ai adresinden indirin ve kurun

2. **Model Ä°ndirin**
   ```bash
   ollama pull llama3.2
   # veya
   ollama pull mistral
   # veya
   ollama pull codellama
   ```

3. **Ollama'yÄ± BaÅŸlatÄ±n**
   ```bash
   ollama serve
   ```

4. **Ortam DeÄŸiÅŸkenlerini AyarlayÄ±n**
   
   Windows:
   ```bash
   set LLM_PROVIDER=ollama
   set OLLAMA_BASE_URL=http://localhost:11434
   set OLLAMA_MODEL=llama3.2
   ```
   
   Linux/Mac:
   ```bash
   export LLM_PROVIDER=ollama
   export OLLAMA_BASE_URL=http://localhost:11434
   export OLLAMA_MODEL=llama3.2
   ```

**Not:** Sistem varsayÄ±lan olarak Ollama'yÄ± otomatik algÄ±lar ve kullanÄ±r (eÄŸer baÅŸka bir LLM yapÄ±landÄ±rÄ±lmamÄ±ÅŸsa).

## ğŸŒ Dil YÃ¶netimi (2 AÅŸamalÄ±: Analiz + TÃ¼rkÃ§e Ã‡eviri)

Bu projede Ã§Ä±ktÄ±larÄ±n TÃ¼rkÃ§e olmasÄ± iÃ§in iki aÅŸamalÄ± bir akÄ±ÅŸ desteklenir:

- **Ana model (Analiz/Senaryo)**: `LLM_PROVIDER` + `OLLAMA_MODEL` (veya OpenAI/Anthropic)
- **Ä°kinci model (TÃ¼rkÃ§e Ã§eviri)**: `TRANSLATION_OLLAMA_MODEL` (Ollama Ã¼zerinden ayrÄ± bir Ã§aÄŸrÄ±)

Ã–rnek (Windows):

```powershell
set LLM_PROVIDER=ollama
set OLLAMA_MODEL=llama3.2

set TRANSLATION_OLLAMA_MODEL=llama3.2
```

Not: Ä°kinci model ayarlÄ± deÄŸilse sistem Ã§eviri iÃ§in `deep-translator` (Google Translate) ile geriye dÃ¶nÃ¼k bir yedek yol kullanabilir.

## ğŸ§  Knowledge Baseâ€™i ZenginleÅŸtirme ve â€œModeli Uyarlamaâ€

Bu projede â€œeÄŸitmeâ€ ihtiyacÄ±nÄ± azaltmak iÃ§in **RAG** yaklaÅŸÄ±mÄ± kullanÄ±lÄ±r:
- `knowledge_base/` altÄ±ndaki JSON iÃ§erikleri RAG tarafÄ±ndan baÄŸlama eklenir.
- Yeni â€œwhat-ifâ€ yorumlarÄ± iÃ§in `whatif_playbook.json` eklendi.

Ä°sterseniz Ollamaâ€™da enerji danÄ±ÅŸmanÄ± tarzÄ±nÄ± sabitlemek iÃ§in â€œcustom modelâ€ oluÅŸturabilirsiniz (tam fine-tune deÄŸildir; sistem prompt/parametre uyarlamasÄ±dÄ±r):

```powershell
ollama create 3gen-energy-tr -f Modelfile.energy-tr
```

Sonra projede:

```powershell
set OLLAMA_MODEL=3gen-energy-tr
```

### SeÃ§enek 4: LLM Olmadan (Åablon TabanlÄ±)

LLM kurulumu yapmazsanÄ±z, sistem otomatik olarak ÅŸablon tabanlÄ± aÃ§Ä±klamalar kullanÄ±r. Bu durumda:
- âœ… API key gerekmez
- âœ… Ãœcretsizdir
- âœ… Hemen Ã§alÄ±ÅŸÄ±r
- âš ï¸ Daha az esnek ve kiÅŸiselleÅŸtirilmiÅŸ aÃ§Ä±klamalar

## ğŸ”§ YapÄ±landÄ±rma

### Ortam DeÄŸiÅŸkenleri

`.env` dosyasÄ± oluÅŸturun (veya sistem ortam deÄŸiÅŸkenlerini kullanÄ±n):

```bash
# LLM Provider seÃ§imi
LLM_PROVIDER=openai  # veya anthropic, ollama

# OpenAI
OPENAI_API_KEY=sk-...

# Anthropic
ANTHROPIC_API_KEY=sk-ant-...

# Ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2
```

### Kod Ä°Ã§inde KullanÄ±m

```python
from backend.llm_client import get_llm_client

# Otomatik algÄ±lama
llm = get_llm_client()

# Manuel seÃ§im
llm = get_llm_client(provider="openai")
```

## ğŸ“Š Model KarÅŸÄ±laÅŸtÄ±rmasÄ±

| Provider | Model | Maliyet | HÄ±z | Kalite | Yerel |
|----------|-------|---------|-----|--------|-------|
| OpenAI | gpt-4o-mini | DÃ¼ÅŸÃ¼k | HÄ±zlÄ± | YÃ¼ksek | âŒ |
| OpenAI | gpt-4o | Orta | Orta | Ã‡ok YÃ¼ksek | âŒ |
| Anthropic | claude-3-haiku | DÃ¼ÅŸÃ¼k | HÄ±zlÄ± | YÃ¼ksek | âŒ |
| Anthropic | claude-3-opus | YÃ¼ksek | YavaÅŸ | Ã‡ok YÃ¼ksek | âŒ |
| Ollama | llama3.2 | Ãœcretsiz | Orta | Ä°yi | âœ… |
| Ollama | mistral | Ãœcretsiz | HÄ±zlÄ± | Ä°yi | âœ… |

## ğŸ§ª Test Etme

LLM'in Ã§alÄ±ÅŸÄ±p Ã§alÄ±ÅŸmadÄ±ÄŸÄ±nÄ± test etmek iÃ§in:

```bash
python test_system.py
```

API yanÄ±tÄ±nda `"llm_used": true` gÃ¶rÃ¼nÃ¼yorsa LLM baÅŸarÄ±yla kullanÄ±lÄ±yor demektir.

## â“ Sorun Giderme

### "LLM integration is not configured" mesajÄ±
- Ortam deÄŸiÅŸkenlerinin doÄŸru ayarlandÄ±ÄŸÄ±ndan emin olun
- API key'lerin geÃ§erli olduÄŸunu kontrol edin
- Paketlerin yÃ¼klÃ¼ olduÄŸunu doÄŸrulayÄ±n

### Ollama baÄŸlantÄ± hatasÄ±
- Ollama'nÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun: `ollama serve`
- Port 11434'Ã¼n aÃ§Ä±k olduÄŸunu kontrol edin
- Model'in indirildiÄŸini doÄŸrulayÄ±n: `ollama list`

### API rate limit hatasÄ±
- Daha yavaÅŸ bir model kullanÄ±n
- Ä°stekler arasÄ±nda bekleme ekleyin
- API planÄ±nÄ±zÄ± kontrol edin

## ğŸ’¡ Ã–neriler

- **GeliÅŸtirme iÃ§in**: Ollama (Ã¼cretsiz, yerel)
- **Demo iÃ§in**: OpenAI gpt-4o-mini (dÃ¼ÅŸÃ¼k maliyet, iyi kalite)
- **Production iÃ§in**: OpenAI gpt-4o veya Anthropic Claude (en iyi kalite)
